{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3adel/auto-correct/blob/main/training/t5_text_spelling_local_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install transformers"
      ],
      "metadata": {
        "id": "KCdJ5Dx6eudX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "c4E9S7Y8j3Ig"
      },
      "outputs": [],
      "source": [
        "#Setup logging\n",
        "\n",
        "#from google.colab import drive\n",
        "import datetime\n",
        "import logging\n",
        "\n",
        "#drive.mount('/content/drive/')\n",
        "\n",
        "# Function to write logs with timestamp\n",
        "now_log_file_name_suffix = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "log_file_path = f'/Users/adel/adel/dev/playground/auto-correct/logs/{now_log_file_name_suffix}-t5-text_spelling_trainer.log'\n",
        "def log_message(message):\n",
        "    # Get the current date and time\n",
        "    now = datetime.datetime.now()\n",
        "    # Format the current date and time as a string\n",
        "    timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    # Prepare the log entry\n",
        "    log_entry = f\"{timestamp} - {message}\\n\"\n",
        "    # Open the log file and append the log entry\n",
        "    with open(log_file_path, 'a') as file:\n",
        "        file.write(log_entry)\n",
        "\n",
        "# Example usage\n",
        "log_message(\"Execution starts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5evUwjh2PtR2",
        "outputId": "21ab84ff-9dff-442c-a9e2-d7a31f95e936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Load the pre-trained T5-small model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", legacy=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "I2tEuxIkPtR5"
      },
      "outputs": [],
      "source": [
        "# Define the task-specific head\n",
        "task_prefix = \"correct: \"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "edcseI4zqzSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1175d056-281f-43da-b832-18bf424b0fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL pointing directly to the raw CSV data\n",
        "url = 'https://raw.githubusercontent.com/3adel/auto-correct/main/training_sets_dumb/frequency-alpha-alldicts_1000_words.csv'\n",
        "\n",
        "try:\n",
        "    # Attempt to read the CSV file from the provided URL\n",
        "    data = pd.read_csv(url)\n",
        "    print(\"Data loaded successfully!\")\n",
        "except Exception as e:\n",
        "    # Print the error if the file cannot be read\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "training_data = list(zip(data['misspelling'], data['correct_word']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "p0YuymR-PtR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f716b0-4ff0-4804-8fe1-bd7cbf3cab1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# rows in training data touples =  10223\n",
            "Seen data: [('meetign', 'meeting'), (' boedi', 'body'), (' beguinn', 'began'), (' simlar', 'similar'), (' linnes', 'lines'), (' proposd', 'proposed'), (' sensas', 'sense'), (' furthurst', 'further'), ('tel', 'tell'), (' attetion', 'attention'), (' sael', 'sale'), (' constrcution', 'construction'), ('mye', 'my'), (' larig', 'large'), (' cettee', 'set'), (' saund', 'sound'), (' lates', 'late'), (' humaniin', 'human'), (' madee', 'made'), (' hadas', 'had'), (' writen', 'written'), (' reange', 'range'), (' suych', 'such'), (' othre', 'other'), (' otuo', 'out'), (' Wersten', 'Western'), (' plaingtiff', 'plaintiff'), (' othere', 'other'), (' posst', 'post'), (' paa', 'pay'), (' impotantce', 'importance'), (' firie', 'fire'), (' leftey', 'left'), (' twyords', 'toward'), (' suloi', 'soil'), ('ma', 'am'), (' Amerciia', 'America'), (' harid', 'hard'), (' momnet', 'moment'), (' prrioi', 'prior'), (' wiid', 'wide'), ('werre', 'were'), (' throguhout', 'throughout'), (' turtn', 'turn'), (' lttr', 'letter'), (' aprroach', 'approach'), ('Army', 'Army'), (' anotter', 'another'), (' Tabol', 'Table'), (' activiy', 'activity'), (' condtions', 'conditions'), ('Presedent', 'President'), (' majior', 'major'), (' haviing', 'having'), (' percntt', 'percent'), ('middel', 'middle'), (' hizs', 'his'), (' paiedd', 'paid'), (' blacikk', 'black'), ('unnderstand', 'understand'), (' arut', 'art'), (' Frnech', 'French'), (' Jhoen', 'John'), (' datat', 'data'), (' avrege', 'average'), (' wrdoss', 'words'), (' agenst', 'against'), ('himsalf', 'himself'), (' chanhe', 'change'), (' noote', 'note'), (' insurnace', 'insurance'), (' alung', 'along'), (' kownn', 'known'), (' pollicy', 'policy'), (' lviie', 'live'), (' rullis', 'rules'), (' lesse', 'less'), (' lllooked', 'looked'), (' civvil', 'civil'), (' trurst', 'trust'), (' abvove', 'above'), (' weeke', 'week'), (' persnall', 'personal'), (' cuit', 'cut'), ('anser', 'answer'), (' tezt', 'test'), (' sinne', 'seen'), (' thier', 'they'), (' involed', 'involved'), (' theriyo', 'theory'), (' rehther', 'rather'), (' caem', 'came'), (' centry', 'century'), (' acounte', 'account'), (' lenth', 'length'), (' propossed', 'proposed'), (' wommen', 'women'), ('evven', 'even'), ('lengeth', 'length'), (' datat', 'data')]\n"
          ]
        }
      ],
      "source": [
        "# Now text_pairs contains tuples in the format (incorrect, correct)\n",
        "print(\"# rows in training data touples = \",len(training_data))\n",
        "log_message(f\"# rows in training data = {len(training_data)}\")\n",
        "log_message(f\"############ Random seen data to be validated by the model later.\")\n",
        "\n",
        "seen_validation_data = []\n",
        "number_seen_data_points = 100\n",
        "\n",
        "for i in range (number_seen_data_points):\n",
        "  random_range_start_index = random.randrange(1,9050,1)\n",
        "  for row in training_data[random_range_start_index:random_range_start_index+1]:\n",
        "    #print(row[0])\n",
        "    seen_validation_data.append(row)\n",
        "print(f\"Seen data: {seen_validation_data}\")\n",
        "log_message(f\"Seen data: {seen_validation_data}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7yJSJifNN9QU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00006680-db1e-4fd5-b97a-884cc120e6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in unseen validation data = 100\n",
            "Unseen validation data: [('Appel', 'Apple'), ('Booke', 'Book'), ('Kare', 'Car'), ('Dög', 'Dog'), ('Hoise', 'House'), ('Trei', 'Tree'), ('Freind', 'Friend'), ('Hapy', 'Happy'), ('Bieg', 'Big'), ('Littel', 'Little'), ('Rune', 'Run'), ('Jummp', 'Jump'), ('Sée', 'See'), ('Heer', 'Hear'), ('Saiy', 'Say'), ('Goe', 'Go'), ('Comey', 'Come'), ('Stoip', 'Stop'), ('Starit', 'Start'), ('Fynish', 'Finish'), ('Wont', 'Want'), ('Neid', 'Need'), ('Likie', 'Like'), ('Lovve', 'Love'), ('Heelp', 'Help'), ('Tayk', 'Take'), ('Givv', 'Give'), ('Mekke', 'Make'), ('Doe', 'Do'), ('Noow', 'Know'), ('Thinck', 'Think'), ('Fiel', 'Feel'), ('Sée', 'See'), ('Heer', 'Hear'), ('Seee', 'See'), ('Heer', 'Hear'), ('Eit', 'Eat'), ('Drinik', 'Drink'), ('Sleeep', 'Sleep'), ('Wakie', 'Wake'), ('Wurk', 'Work'), ('Plaiy', 'Play'), ('Lernn', 'Learn'), ('Readd', 'Read'), ('Rrite', 'Write'), ('Tawk', 'Talk'), ('Lissten', 'Listen'), ('Smilie', 'Smile'), ('Laught', 'Laugh'), ('Crie', 'Cry'), ('Yelle', 'Yell'), ('Hurrie', 'Hurry'), ('Waite', 'Wait'), ('Hoipe', 'Hope'), ('Wisshe', 'Wish'), ('Trii', 'Try'), ('Fale', 'Fail'), ('Suceede', 'Succeed'), ('Begn', 'Begin'), ('Ennd', 'End'), ('Mornig', 'Morning'), ('Afteernoon', 'Afternoon'), ('Eveninng', 'Evening'), ('Nite', 'Night'), ('Dei', 'Day'), ('Wieck', 'Week'), ('Montn', 'Month'), ('Yeer', 'Year'), ('Centurie', 'Century'), ('Lief', 'Life'), ('Tymme', 'Time'), ('Spase', 'Space'), ('Energee', 'Energy'), ('Watair', 'Water'), ('Firre', 'Fire'), ('Aire', 'Air'), ('Erth', 'Earth'), ('Skye', 'Sky'), ('Sone', 'Sun'), ('Munne', 'Moon'), ('Phenomonal', 'Phenomenal'), ('Compitition', 'Competition'), ('Independacently', 'Independently'), ('Unrelevent', 'Irrelevant'), ('Disapointment', 'Disappointment'), ('Requirments', 'Requirements'), ('Impossiblity', 'Impossibility'), ('Exsceptional', 'Exceptional'), ('Incurrable', 'Incurred'), ('Unnoticable', 'Unnoticeable'), ('Misunderstnding', 'Misunderstanding'), ('Reccomendation', 'Recommendation'), ('Incompatiblity', 'Incompatibility'), ('Exsistential', 'Existential'), ('Irrelevent', 'Irrelevant'), ('Unpredicatable', 'Unpredictable'), ('Disastorous', 'Disastrous'), ('Accomplishement', 'Accomplishment'), ('Inefficiency', 'Inefficacy'), ('Impracticality', 'Impracticability')]\n"
          ]
        }
      ],
      "source": [
        "#Load unseen data by the model\n",
        "unseen_validation_data = [\n",
        "    (\"Appel\", \"Apple\"),\n",
        "    (\"Booke\", \"Book\"),\n",
        "    (\"Kare\", \"Car\"),\n",
        "    (\"Dög\", \"Dog\"),\n",
        "    (\"Hoise\", \"House\"),\n",
        "    (\"Trei\", \"Tree\"),\n",
        "    (\"Freind\", \"Friend\"),\n",
        "    (\"Hapy\", \"Happy\"),\n",
        "    (\"Bieg\", \"Big\"),\n",
        "    (\"Littel\", \"Little\"),\n",
        "    (\"Rune\", \"Run\"),\n",
        "    (\"Jummp\", \"Jump\"),\n",
        "    (\"Sée\", \"See\"),\n",
        "    (\"Heer\", \"Hear\"),\n",
        "    (\"Saiy\", \"Say\"),\n",
        "    (\"Goe\", \"Go\"),\n",
        "    (\"Comey\", \"Come\"),\n",
        "    (\"Stoip\", \"Stop\"),\n",
        "    (\"Starit\", \"Start\"),\n",
        "    (\"Fynish\", \"Finish\"),\n",
        "    (\"Wont\", \"Want\"),\n",
        "    (\"Neid\", \"Need\"),\n",
        "    (\"Likie\", \"Like\"),\n",
        "    (\"Lovve\", \"Love\"),\n",
        "    (\"Heelp\", \"Help\"),\n",
        "    (\"Tayk\", \"Take\"),\n",
        "    (\"Givv\", \"Give\"),\n",
        "    (\"Mekke\", \"Make\"),\n",
        "    (\"Doe\", \"Do\"),\n",
        "    (\"Noow\", \"Know\"),\n",
        "    (\"Thinck\", \"Think\"),\n",
        "    (\"Fiel\", \"Feel\"),\n",
        "    (\"Sée\", \"See\"),\n",
        "    (\"Heer\", \"Hear\"),\n",
        "    (\"Seee\", \"See\"),\n",
        "    (\"Heer\", \"Hear\"),\n",
        "    (\"Eit\", \"Eat\"),\n",
        "    (\"Drinik\", \"Drink\"),\n",
        "    (\"Sleeep\", \"Sleep\"),\n",
        "    (\"Wakie\", \"Wake\"),\n",
        "    (\"Wurk\", \"Work\"),\n",
        "    (\"Plaiy\", \"Play\"),\n",
        "    (\"Lernn\", \"Learn\"),\n",
        "    (\"Readd\", \"Read\"),\n",
        "    (\"Rrite\", \"Write\"),\n",
        "    (\"Tawk\", \"Talk\"),\n",
        "    (\"Lissten\", \"Listen\"),\n",
        "    (\"Smilie\", \"Smile\"),\n",
        "    (\"Laught\", \"Laugh\"),\n",
        "    (\"Crie\", \"Cry\"),\n",
        "    (\"Yelle\", \"Yell\"),\n",
        "    (\"Hurrie\", \"Hurry\"),\n",
        "    (\"Waite\", \"Wait\"),\n",
        "    (\"Hoipe\", \"Hope\"),\n",
        "    (\"Wisshe\", \"Wish\"),\n",
        "    (\"Trii\", \"Try\"),\n",
        "    (\"Fale\", \"Fail\"),\n",
        "    (\"Suceede\", \"Succeed\"),\n",
        "    (\"Begn\", \"Begin\"),\n",
        "    (\"Ennd\", \"End\"),\n",
        "    (\"Mornig\", \"Morning\"),\n",
        "    (\"Afteernoon\", \"Afternoon\"),\n",
        "    (\"Eveninng\", \"Evening\"),\n",
        "    (\"Nite\", \"Night\"),\n",
        "    (\"Dei\", \"Day\"),\n",
        "    (\"Wieck\", \"Week\"),\n",
        "    (\"Montn\", \"Month\"),\n",
        "    (\"Yeer\", \"Year\"),\n",
        "    (\"Centurie\", \"Century\"),\n",
        "    (\"Lief\", \"Life\"),\n",
        "    (\"Tymme\", \"Time\"),\n",
        "    (\"Spase\", \"Space\"),\n",
        "    (\"Energee\", \"Energy\"),\n",
        "    (\"Watair\", \"Water\"),\n",
        "    (\"Firre\", \"Fire\"),\n",
        "    (\"Aire\", \"Air\"),\n",
        "    (\"Erth\", \"Earth\"),\n",
        "    (\"Skye\", \"Sky\"),\n",
        "    (\"Sone\", \"Sun\"),\n",
        "    (\"Munne\", \"Moon\"),\n",
        "    (\"Phenomonal\", \"Phenomenal\"),\n",
        "    (\"Compitition\", \"Competition\"),\n",
        "    (\"Independacently\", \"Independently\"),\n",
        "    (\"Unrelevent\", \"Irrelevant\"),\n",
        "    (\"Disapointment\", \"Disappointment\"),\n",
        "    (\"Requirments\", \"Requirements\"),\n",
        "    (\"Impossiblity\", \"Impossibility\"),\n",
        "    (\"Exsceptional\", \"Exceptional\"),\n",
        "    (\"Incurrable\", \"Incurred\"),\n",
        "    (\"Unnoticable\", \"Unnoticeable\"),\n",
        "    (\"Misunderstnding\", \"Misunderstanding\"),\n",
        "    (\"Reccomendation\", \"Recommendation\"),\n",
        "    (\"Incompatiblity\", \"Incompatibility\"),\n",
        "    (\"Exsistential\", \"Existential\"),\n",
        "    (\"Irrelevent\", \"Irrelevant\"),\n",
        "    (\"Unpredicatable\", \"Unpredictable\"),\n",
        "    (\"Disastorous\", \"Disastrous\"),\n",
        "    (\"Accomplishement\", \"Accomplishment\"),\n",
        "    (\"Inefficiency\", \"Inefficacy\"),\n",
        "    (\"Impracticality\", \"Impracticability\")\n",
        "]\n",
        "\n",
        "log_message(f\"############ Unseen data to be validated by the model later.\")\n",
        "print(f\"Number of rows in unseen validation data = {len(unseen_validation_data)}\")\n",
        "log_message(f\"Number of rows in unseen validation data = {len(unseen_validation_data)}\")\n",
        "\n",
        "\n",
        "print(f\"Unseen validation data: {unseen_validation_data}\")\n",
        "log_message(f\"Unawwn validation data: {unseen_validation_data}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mxaML4a-ecLd"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "def preprocess_data(input_texts, target_texts):\n",
        "\n",
        "    #To address the issue of potential non-string types and None values in your input_texts and target_texts\n",
        "    inputs = [task_prefix + str(text) for text in input_texts if text is not None]\n",
        "    target_texts = [str(text) for text in target_texts if text is not None]\n",
        "\n",
        "    # Tokenize the inputs and targets\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    labels = tokenizer(target_texts, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "    return model_inputs, labels\n",
        "\n",
        "# Prepare the training data\n",
        "input_texts, target_texts = zip(*training_data)\n",
        "train_inputs, train_labels = preprocess_data(input_texts, target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UtXGi9wTgMIA"
      },
      "outputs": [],
      "source": [
        "# Inference method\n",
        "def correct_text(input_text):\n",
        "    input_ids = tokenizer.encode(task_prefix + input_text, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(input_ids, max_length=512)[0]\n",
        "    corrected_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "    return corrected_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HBC2_GQ-goQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12deb5d3-5163-4dc6-d895-ac01fb76868f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:meetign--->meetign\n",
            "1: boedi--->correct: boedi\n",
            "2: beguinn--->: beguinn\n",
            "3: simlar--->: simlar\n",
            "4: linnes--->-\n",
            "5: proposd--->correct: proposition\n",
            "6: sensas--->correct: sensas\n",
            "7: furthurst--->: furthurst\n",
            "8:tel--->correct: tel\n",
            "9: attetion--->correct: attetion\n",
            "10: sael--->-\n",
            "11: constrcution--->: constrcution\n",
            "12:mye--->: mye\n",
            "13: larig--->correct: larig\n",
            "14: cettee--->correct: cettee\n",
            "15: saund--->correct: saund\n",
            "16: lates--->Richtig: lates\n",
            "17: humaniin--->: humaniin\n",
            "18: madee--->correct: madee\n",
            "19: hadas--->- -\n",
            "20: writen--->correct: writen\n",
            "21: reange--->reange\n",
            "22: suych--->suych\n",
            "23: othre--->othre\n",
            "24: otuo--->correct: otuo\n",
            "25: Wersten--->Wersten: Wersten: Wersten: Wersten: Wersten: Wersten Wersten: Wersten: Wersten: Wersten\n",
            "26: plaingtiff--->: plaingtiff\n",
            "27: othere--->correct: correct: othere\n",
            "28: posst--->Richtig: Posst\n",
            "29: paa--->correct: paa\n",
            "30: impotantce--->correct: impotantce\n",
            "31: firie--->: firie\n",
            "32: leftey--->leftey\n",
            "33: twyords--->twyords\n",
            "34: suloi--->suloi\n",
            "35:ma--->correct: ma\n",
            "36: Amerciia--->correct: Amerciia\n",
            "37: harid--->correct: harid\n",
            "38: momnet--->momnet\n",
            "39: prrioi--->correct: prrioi\n",
            "40: wiid--->correct: wiid\n",
            "41:werre--->correct correct: werre\n",
            "42: throguhout--->throguhout\n",
            "43: turtn--->turtn\n",
            "44: lttr--->correct: lttr\n",
            "45: aprroach--->correct:\n",
            "46:Army--->: Army\n",
            "47: anotter--->correct: anotter\n",
            "48: Tabol--->Tabol\n",
            "49: activiy--->correct: activiy\n",
            "50: condtions--->correct: condtions\n",
            "51:Presedent--->Presedent\n",
            "52: majior--->correct:\n",
            "53: haviing--->correct: haviing\n",
            "54: percntt--->correct: percntt\n",
            "55:middel--->correct: middel\n",
            "56: hizs--->Richtig: hizs\n",
            "57: paiedd--->correct: paiedd\n",
            "58: blacikk--->correct: blacikk\n",
            "59:unnderstand--->: unnderstand\n",
            "60: arut--->correct: arut\n",
            "61: Frnech--->Frnech\n",
            "62: Jhoen--->correct correct: Jhoen\n",
            "63: datat--->: datat\n",
            "64: avrege--->correct: avrege\n",
            "65: wrdoss--->wrdoss\n",
            "66: agenst--->Richtig: agenst\n",
            "67:himsalf--->\n",
            "68: chanhe--->correct: chanhe\n",
            "69: noote--->\n",
            "70: insurnace--->: insurnace\n",
            "71: alung--->correct: alung\n",
            "72: kownn--->correct correct: kownn\n",
            "73: pollicy--->: pollicy\n",
            "74: lviie--->lviie\n",
            "75: rullis--->correct: rullis\n",
            "76: lesse--->: lesse\n",
            "77: lllooked--->correct: lllooked\n",
            "78: civvil--->civvil\n",
            "79: trurst--->\n",
            "80: abvove--->: abvove\n",
            "81: weeke--->correct: weeke\n",
            "82: persnall--->-\n",
            "83: cuit--->correct: correct: cuit\n",
            "84:anser--->Richtig: anser\n",
            "85: tezt--->correct: tezt\n",
            "86: sinne--->\n",
            "87: thier--->correct: thier\n",
            "88: involed--->correct: involed\n",
            "89: theriyo--->correct: theriyo\n",
            "90: rehther--->correct: rehther\n",
            "91: caem--->correct: caem\n",
            "92: centry--->correct: centry\n",
            "93: acounte--->correct: correct\n",
            "94: lenth--->correct: lenth\n",
            "95: propossed--->correct: proposition\n",
            "96: wommen--->\n",
            "97:evven--->: evven\n",
            "98:lengeth--->: lengeth\n",
            "99: datat--->: datat\n",
            "CPU times: user 46.8 s, sys: 2min 34s, total: 3min 21s\n",
            "Wall time: 19.3 s\n"
          ]
        }
      ],
      "source": [
        "#run inference BEFORE finetuning against SEEN data\n",
        "%%time\n",
        "log_message(f\"############ run inference BEFORE finetuning against SEEN data\")\n",
        "for i, row in enumerate(seen_validation_data):\n",
        "  correct_text(row[0])\n",
        "  print(f\"{i}:{row[0]}--->{correct_text(row[0])}\")\n",
        "  log_message(f\"{i}:{row[0]}--->{correct_text(row[0])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ueauTWNciiMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907283e3-5811-49a2-e21b-1a9fdecbd450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:Appel--->Appel\n",
            "1:Booke--->Booke\n",
            "2:Kare--->Kare\n",
            "3:Dög--->Dög: Dög Dög: Dög Dög: Dög Dög: Dög: Dög: Dög: Dög: Dög: Dög\n",
            "4:Hoise--->Hoise\n",
            "5:Trei--->correct: Trei\n",
            "6:Freind--->Freind: Freind\n",
            "7:Hapy--->Hapy: Hapy\n",
            "8:Bieg--->Bieg\n",
            "9:Littel--->Littel Littel Littel: Littel Littel Littel Littel Littel: Littel\n",
            "10:Rune--->Rune\n",
            "11:Jummp--->Jummp: Jummp Jummp: Jummp Jummp Jummp Jummp: Jummp: Jummp: Jummp Jummp: Jummp\n",
            "12:Sée--->Sé\n",
            "13:Heer--->Heer: Heer: Heer: Heer Heer: Heer\n",
            "14:Saiy--->Saiy\n",
            "15:Goe--->Goe: Goe: Goe: Goe Goe: Goe Goe: Goe: Goe: Goe: Goe: Goe: Goe\n",
            "16:Comey--->Comey\n",
            "17:Stoip--->Stoip\n",
            "18:Starit--->Starit: Starit\n",
            "19:Fynish--->Fynish: Fynish: Fynish: Fynish Fynish: Fynish\n",
            "20:Wont--->Wont\n",
            "21:Neid--->correct: Neid\n",
            "22:Likie--->Likie\n",
            "23:Lovve--->: Lovve\n",
            "24:Heelp--->Heelp\n",
            "25:Tayk--->Tayk\n",
            "26:Givv--->Givv\n",
            "27:Mekke--->Mekke\n",
            "28:Doe--->Doe\n",
            "29:Noow--->Noow\n",
            "30:Thinck--->: Thinck\n",
            "31:Fiel--->Fiel\n",
            "32:Sée--->Sé\n",
            "33:Heer--->Heer: Heer: Heer: Heer Heer: Heer\n",
            "34:Seee--->Seee\n",
            "35:Heer--->Heer: Heer: Heer: Heer Heer: Heer\n",
            "36:Eit--->Eit: Eit: Eit: Eit: Eit: Eit\n",
            "37:Drinik--->correct: Drinik\n",
            "38:Sleeep--->correct: Sleep\n",
            "39:Wakie--->: Wakie\n",
            "40:Wurk--->: Wurk\n",
            "41:Plaiy--->correct: Plaiy\n",
            "42:Lernn--->Lernn\n",
            "43:Readd--->readd\n",
            "44:Rrite--->correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct\n",
            "45:Tawk--->: Tawk\n",
            "46:Lissten--->Richtig: Lissten\n",
            "47:Smilie--->Smilie\n",
            "48:Laught--->Laught: Laught Laught: Laught Laught: Laught\n",
            "49:Crie--->Crie\n",
            "50:Yelle--->Yelle\n",
            "51:Hurrie--->: Hurrie\n",
            "52:Waite--->: Waite\n",
            "53:Hoipe--->: Hoipe\n",
            "54:Wisshe--->Richtig: Wisshe\n",
            "55:Trii--->a : Trii\n",
            "56:Fale--->False\n",
            "57:Suceede--->Suceede\n",
            "58:Begn--->Begn\n",
            "59:Ennd--->correct correct: Ennd\n",
            "60:Mornig--->Mornig\n",
            "61:Afteernoon--->Richtig: Afteerno\n",
            "62:Eveninng--->Eveninng\n",
            "63:Nite--->Nite\n",
            "64:Dei--->correct: Dei\n",
            "65:Wieck--->Wieck\n",
            "66:Montn--->Montn\n",
            "67:Yeer--->correct: Yeer\n",
            "68:Centurie--->Centurie\n",
            "69:Lief--->Lief\n",
            "70:Tymme--->: Tymme\n",
            "71:Spase--->Richtig: Spase\n",
            "72:Energee--->correct: Energee\n",
            "73:Watair--->Watair\n",
            "74:Firre--->Firre\n",
            "75:Aire--->Aire\n",
            "76:Erth--->Erth\n",
            "77:Skye--->Skye\n",
            "78:Sone--->Sone\n",
            "79:Munne--->Munne\n",
            "80:Phenomonal--->correct correct: Phenomonal\n",
            "81:Compitition--->Compitition\n",
            "82:Independacently--->correct:\n",
            "83:Unrelevent--->Unrelevent\n",
            "84:Disapointment--->Désignation\n",
            "85:Requirments--->Kor correct: Exquirments\n",
            "86:Impossiblity--->Richtig: Impossiblity\n",
            "87:Exsceptional--->Richtig: Exsceptional\n",
            "88:Incurrable--->: Incurrable\n",
            "89:Unnoticable--->Unnoticable\n",
            "90:Misunderstnding--->Richtig: Fehl\n",
            "91:Reccomendation--->Reccomendation\n",
            "92:Incompatiblity--->Incompatiblity\n",
            "93:Exsistential--->Richtig: Exsistential\n",
            "94:Irrelevent--->Irrelevent\n",
            "95:Unpredicatable--->Unpredictable\n",
            "96:Disastorous--->: Disastorous\n",
            "97:Accomplishement--->Kor\n",
            "98:Inefficiency--->Ineffizienz\n",
            "99:Impracticality--->Impractical\n",
            "CPU times: user 49.6 s, sys: 2min 52s, total: 3min 41s\n",
            "Wall time: 20.2 s\n"
          ]
        }
      ],
      "source": [
        "#run inference BEFORE finetuning against UNSEEN data\n",
        "%%time\n",
        "log_message(f\"############ run inference BEFORE finetuning against UNSEEN data\")\n",
        "for i, row in enumerate(unseen_validation_data):\n",
        "  correct_text(row[0])\n",
        "  print(f\"{i}:{row[0]}--->{correct_text(row[0])}\")\n",
        "  log_message(f\"{i}:{row[0]}--->{correct_text(row[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qGHMV6ZsPtR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b135a327-c91b-4b14-8eb8-396cc9b2ce3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training rows per batch: 639\n"
          ]
        }
      ],
      "source": [
        "#fine tuning w/ batch procerssing\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset = TensorDataset(train_inputs[\"input_ids\"], train_inputs[\"attention_mask\"], train_labels)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 16  # You can adjust the batch size according to your GPU memory\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "print(\"# training rows per batch:\", len(train_dataloader))\n",
        "log_message(f\"# training rows per batch = {len(train_dataloader)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyRyeMxePtR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3ae642-55d8-42a6-ecb6-993283aeed89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-30 13:33:43 Started batch #1/639 in epoch #1/2\n",
            "2024-04-30 13:33:53 Finished batch #1/639 in epoch #1/2 in (9.56 seconds)\n",
            "2024-04-30 13:33:53 Started batch #2/639 in epoch #1/2\n",
            "2024-04-30 13:34:02 Finished batch #2/639 in epoch #1/2 in (9.41 seconds)\n",
            "2024-04-30 13:34:02 Started batch #3/639 in epoch #1/2\n",
            "2024-04-30 13:34:11 Finished batch #3/639 in epoch #1/2 in (9.35 seconds)\n",
            "2024-04-30 13:34:11 Started batch #4/639 in epoch #1/2\n",
            "2024-04-30 13:34:21 Finished batch #4/639 in epoch #1/2 in (9.36 seconds)\n",
            "2024-04-30 13:34:21 Started batch #5/639 in epoch #1/2\n",
            "2024-04-30 13:34:30 Finished batch #5/639 in epoch #1/2 in (9.38 seconds)\n",
            "2024-04-30 13:34:30 Started batch #6/639 in epoch #1/2\n",
            "2024-04-30 13:34:39 Finished batch #6/639 in epoch #1/2 in (9.36 seconds)\n",
            "2024-04-30 13:34:39 Started batch #7/639 in epoch #1/2\n",
            "2024-04-30 13:34:49 Finished batch #7/639 in epoch #1/2 in (9.34 seconds)\n",
            "2024-04-30 13:34:49 Started batch #8/639 in epoch #1/2\n",
            "2024-04-30 13:34:58 Finished batch #8/639 in epoch #1/2 in (9.43 seconds)\n",
            "2024-04-30 13:34:58 Started batch #9/639 in epoch #1/2\n",
            "2024-04-30 13:35:08 Finished batch #9/639 in epoch #1/2 in (9.36 seconds)\n",
            "2024-04-30 13:35:08 Started batch #10/639 in epoch #1/2\n",
            "2024-04-30 13:35:17 Finished batch #10/639 in epoch #1/2 in (9.35 seconds)\n",
            "2024-04-30 13:35:17 Started batch #11/639 in epoch #1/2\n",
            "2024-04-30 13:35:26 Finished batch #11/639 in epoch #1/2 in (9.29 seconds)\n",
            "2024-04-30 13:35:26 Started batch #12/639 in epoch #1/2\n",
            "2024-04-30 13:35:36 Finished batch #12/639 in epoch #1/2 in (9.31 seconds)\n",
            "2024-04-30 13:35:36 Started batch #13/639 in epoch #1/2\n",
            "2024-04-30 13:35:45 Finished batch #13/639 in epoch #1/2 in (9.32 seconds)\n",
            "2024-04-30 13:35:45 Started batch #14/639 in epoch #1/2\n",
            "2024-04-30 13:35:54 Finished batch #14/639 in epoch #1/2 in (9.36 seconds)\n",
            "2024-04-30 13:35:54 Started batch #15/639 in epoch #1/2\n",
            "2024-04-30 13:36:04 Finished batch #15/639 in epoch #1/2 in (9.31 seconds)\n",
            "2024-04-30 13:36:04 Started batch #16/639 in epoch #1/2\n",
            "2024-04-30 13:36:13 Finished batch #16/639 in epoch #1/2 in (9.49 seconds)\n",
            "2024-04-30 13:36:13 Started batch #17/639 in epoch #1/2\n",
            "2024-04-30 13:36:22 Finished batch #17/639 in epoch #1/2 in (9.38 seconds)\n",
            "2024-04-30 13:36:22 Started batch #18/639 in epoch #1/2\n",
            "2024-04-30 13:36:32 Finished batch #18/639 in epoch #1/2 in (9.54 seconds)\n",
            "2024-04-30 13:36:32 Started batch #19/639 in epoch #1/2\n",
            "2024-04-30 13:36:41 Finished batch #19/639 in epoch #1/2 in (9.45 seconds)\n",
            "2024-04-30 13:36:41 Started batch #20/639 in epoch #1/2\n",
            "2024-04-30 13:36:51 Finished batch #20/639 in epoch #1/2 in (9.47 seconds)\n",
            "2024-04-30 13:36:51 Started batch #21/639 in epoch #1/2\n",
            "2024-04-30 13:37:00 Finished batch #21/639 in epoch #1/2 in (9.41 seconds)\n",
            "2024-04-30 13:37:00 Started batch #22/639 in epoch #1/2\n",
            "2024-04-30 13:37:10 Finished batch #22/639 in epoch #1/2 in (9.48 seconds)\n",
            "2024-04-30 13:37:10 Started batch #23/639 in epoch #1/2\n",
            "2024-04-30 13:37:19 Finished batch #23/639 in epoch #1/2 in (9.46 seconds)\n",
            "2024-04-30 13:37:19 Started batch #24/639 in epoch #1/2\n",
            "2024-04-30 13:37:29 Finished batch #24/639 in epoch #1/2 in (9.46 seconds)\n",
            "2024-04-30 13:37:29 Started batch #25/639 in epoch #1/2\n",
            "2024-04-30 13:37:38 Finished batch #25/639 in epoch #1/2 in (9.30 seconds)\n",
            "2024-04-30 13:37:38 Started batch #26/639 in epoch #1/2\n",
            "2024-04-30 13:37:47 Finished batch #26/639 in epoch #1/2 in (9.29 seconds)\n",
            "2024-04-30 13:37:47 Started batch #27/639 in epoch #1/2\n",
            "2024-04-30 13:37:57 Finished batch #27/639 in epoch #1/2 in (9.28 seconds)\n",
            "2024-04-30 13:37:57 Started batch #28/639 in epoch #1/2\n",
            "2024-04-30 13:38:06 Finished batch #28/639 in epoch #1/2 in (9.39 seconds)\n",
            "2024-04-30 13:38:06 Started batch #29/639 in epoch #1/2\n",
            "2024-04-30 13:38:15 Finished batch #29/639 in epoch #1/2 in (9.31 seconds)\n",
            "2024-04-30 13:38:15 Started batch #30/639 in epoch #1/2\n",
            "2024-04-30 13:38:25 Finished batch #30/639 in epoch #1/2 in (9.47 seconds)\n",
            "2024-04-30 13:38:25 Started batch #31/639 in epoch #1/2\n",
            "2024-04-30 13:38:34 Finished batch #31/639 in epoch #1/2 in (9.36 seconds)\n",
            "2024-04-30 13:38:34 Started batch #32/639 in epoch #1/2\n",
            "2024-04-30 13:38:43 Finished batch #32/639 in epoch #1/2 in (9.34 seconds)\n",
            "2024-04-30 13:38:43 Started batch #33/639 in epoch #1/2\n",
            "2024-04-30 13:38:53 Finished batch #33/639 in epoch #1/2 in (9.34 seconds)\n",
            "2024-04-30 13:38:53 Started batch #34/639 in epoch #1/2\n",
            "2024-04-30 13:39:02 Finished batch #34/639 in epoch #1/2 in (9.29 seconds)\n",
            "2024-04-30 13:39:02 Started batch #35/639 in epoch #1/2\n",
            "2024-04-30 13:39:11 Finished batch #35/639 in epoch #1/2 in (9.35 seconds)\n",
            "2024-04-30 13:39:11 Started batch #36/639 in epoch #1/2\n",
            "2024-04-30 13:39:21 Finished batch #36/639 in epoch #1/2 in (9.58 seconds)\n",
            "2024-04-30 13:39:21 Started batch #37/639 in epoch #1/2\n",
            "2024-04-30 13:39:30 Finished batch #37/639 in epoch #1/2 in (9.41 seconds)\n",
            "2024-04-30 13:39:30 Started batch #38/639 in epoch #1/2\n",
            "2024-04-30 13:39:40 Finished batch #38/639 in epoch #1/2 in (9.35 seconds)\n",
            "2024-04-30 13:39:40 Started batch #39/639 in epoch #1/2\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune the model w/ frequent output\n",
        "#%%time\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    current_batch = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch_start_time = datetime.datetime.now()\n",
        "        print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Started batch #{current_batch+1}/{len(train_dataloader)} in epoch #{epoch+1}/{num_epochs}\")\n",
        "        log_message(f\"Started batch #{current_batch+1}/{len(train_dataloader)} in epoch #{epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Unpack the batch\n",
        "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
        "\n",
        "\n",
        "        # Zero the gradients on each iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
        "\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        batch_end_time = datetime.datetime.now()\n",
        "        print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Finished batch #{current_batch+1}/{len(train_dataloader)} in epoch #{epoch+1}/{num_epochs} in ({(batch_end_time - batch_start_time).total_seconds():.2f} seconds)\")\n",
        "        log_message(f\"Finished batch #{current_batch+1}/{len(train_dataloader)} in epoch #{epoch+1}/{num_epochs}  in ({(batch_end_time - batch_start_time).total_seconds():.2f} seconds)\")\n",
        "\n",
        "        current_batch += 1\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate average loss over the number of batches\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "    log_message(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW7hjDL6mPmN"
      },
      "outputs": [],
      "source": [
        "#run inference AFTER finetuning against SEEN data\n",
        "%%time\n",
        "log_message(f\"############ run inference AFTER finetuning against SEEN data\")\n",
        "for i, row in enumerate(seen_validation_data):\n",
        "  correct_text(row[0])\n",
        "  print(f\"{i}:{row[0]}--->{correct_text(row[0])}\")\n",
        "  log_message(f\"{i}:{row[0]}--->{correct_text(row[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIt17-R-PtR6"
      },
      "outputs": [],
      "source": [
        "#run inference AFTER finetuning against UNSEEN data\n",
        "%%time\n",
        "log_message(f\"############ run inference AFTER finetuning against UNSEEN data\")\n",
        "for i, row in enumerate(unseen_validation_data):\n",
        "  correct_text(row[0])\n",
        "  print(f\"{i}:{row[0]}--->{correct_text(row[0])}\")\n",
        "  log_message(f\"{i}:{row[0]}--->{correct_text(row[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB_IHn2QoPoF"
      },
      "outputs": [],
      "source": [
        "# Save the model and tokenizer\n",
        "#Save model to Local Drive\n",
        "finetuning_iteration = \"7\"\n",
        "model.save_pretrained(f\"/Users/adel/adel/dev/playground/auto-correct/models/{finetuning_iteration}\")\n",
        "tokenizer.save_pretrained(f\"/Users/adel/adel/dev/playground/auto-correct/models/{finetuning_iteration}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1H18epToXfT"
      },
      "outputs": [],
      "source": [
        "# How to load the model from drive\n",
        "\n",
        "finetuned_model_path = f\"/Users/adel/adel/dev/playground/auto-correct/models/{finetuning_iteration}\"\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(finetuned_model_path)\n",
        "model_finetuned_spelling_en_1000_words = T5ForConditionalGeneration.from_pretrained(finetuned_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD6iJhKDpwg2"
      },
      "outputs": [],
      "source": [
        "#use the finetuned model.\n",
        "def correct_spelling(word):\n",
        "    input_ids = tokenizer.encode(\"spelling correction:\" + word, return_tensors=\"pt\")\n",
        "\n",
        "    # Correct spelling\n",
        "    outputs = model_finetuned_spelling_en_1000_words.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the output\n",
        "    correction = finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return correction\n",
        "\n",
        "# Example text\n",
        "word = \"objeccts\"\n",
        "correction = correct_spelling(word)\n",
        "print(\"Correction:\", correction)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}