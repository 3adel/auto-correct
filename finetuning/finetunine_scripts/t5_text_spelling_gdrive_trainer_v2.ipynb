{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3adel/auto-correct/blob/main/t5_text_spelling_gdrive_trainer_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install transformers"
      ],
      "metadata": {
        "id": "KCdJ5Dx6eudX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4E9S7Y8j3Ig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc52f64b-db3b-4a52-ff9e-379be14c5acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Setup logging\n",
        "\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "import logging\n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 5\n",
        "finetuning_iteration = \"10\"\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Function to write logs with timestamp\n",
        "now_log_file_name_suffix = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "log_file_path = f'/content/drive/My Drive/Startup Projects/auto-correct/logs/{now_log_file_name_suffix}-t5-text_spelling_trainer_v{finetuning_iteration}.log'\n",
        "def log_message(message):\n",
        "    # Get the current date and time\n",
        "    now = datetime.datetime.now()\n",
        "    # Format the current date and time as a string\n",
        "    timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    # Prepare the log entry\n",
        "    log_entry = f\"{timestamp} - {message}\\n\"\n",
        "    # Open the log file and append the log entry\n",
        "    with open(log_file_path, 'a') as file:\n",
        "        file.write(log_entry)\n",
        "\n",
        "# Example usage\n",
        "log_message(\"Execution starts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5evUwjh2PtR2",
        "outputId": "d36b7932-766e-4f67-83f0-0c01b143a7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Load the pre-trained T5-small model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", legacy=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2tEuxIkPtR5"
      },
      "outputs": [],
      "source": [
        "# Define the task-specific head\n",
        "task_prefix = \"correct: \"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edcseI4zqzSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78acbc3c-8fb0-4d79-f763-ef6c446b0d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL pointing directly to the raw CSV data\n",
        "#url = 'https://raw.githubusercontent.com/3adel/auto-correct/main/training_sets_dumb/frequency-alpha-alldicts_1000_words.csv'\n",
        "url = 'https://raw.githubusercontent.com/3adel/auto-correct/main/training_sets_dumb/frequency-alpha-alldicts_4000%60_words.csv'\n",
        "\n",
        "try:\n",
        "    # Attempt to read the CSV file from the provided URL\n",
        "    data = pd.read_csv(url)\n",
        "    print(\"Data loaded successfully!\")\n",
        "except Exception as e:\n",
        "    # Print the error if the file cannot be read\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "training_data = list(zip(data['misspelling'], data['correct_word']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0YuymR-PtR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b496d050-28c5-487f-a755-76acf536acde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# rows in training data tuples =  41232\n",
            "Seen data: [(' Ameriacs', 'America'), (' countris', 'counties'), (' deie', 'die'), (' advnace', 'advance'), (' secuere', 'secure'), (' Brtitan', 'Britain'), (' polisse', 'police'), (' suhhc', 'such'), (' disucss', 'discuss'), (' satifay', 'safety'), (' facilatie', 'facility'), (' Wislon', 'Wilson'), (' casjh', 'cash'), (' raidiatin', 'radiation'), (' eductaional', 'educational'), (' extnsion', 'extension'), (' librayes', 'libraries'), (' Bulletinn', 'Bulletin'), ('Parelement', 'Parliament'), (' flught', 'fight'), (' foerst', 'forest'), (' produxe', 'produce'), ('weither', 'whether'), (' Afreican', 'African'), (' Treasurry', 'Treasury'), (' patern', 'patent'), (' rootts', 'roots'), (' hpoop', 'hope'), (' temie', 'times'), (' Devision', 'Division'), (' jobj', 'job'), (' carrided', 'carried'), (' evniing', 'evening'), (' creatiin', 'creation'), (' quarteer', 'quarter'), (' resltuing', 'resulting'), (' majoirty', 'majority'), (' subsescton', 'subsection'), (' woidl', 'would'), (' findnings', 'findings'), (' ellicton', 'election'), (' townees', 'towns'), (' pasend', 'passed'), (' acceess', 'access'), (' supreoir', 'superior'), (' assissment', 'assessment'), (' seeingg', 'seeing'), (' proccese', 'process'), (' Chiicago', 'Chicago'), (' wuod', 'wood'), (' pathe', 'path'), (' adedd', 'added'), (' themselfe', 'themselves'), (' councel', 'counsel'), (' inernal', 'internal'), (' welldo', 'well'), (' publcation', 'publication'), (' serius', 'series'), (' tabls', 'tables'), (' xpect', 'expect'), (' tethe', 'teeth'), ('fayth', 'faith'), (' mxied', 'mixed'), (' praxise', 'practice'), (' leeader', 'leader'), (' filmin', 'film'), (' aimee', 'aim'), (' throguhlly', 'thoroughly'), (' srent', 'spent'), (' ocupationn', 'occupation'), (' subscection', 'subsection'), (' dennyed', 'denied'), (' becomne', 'become'), ('luiking', 'looking'), (' establisjing', 'establishing'), (' chien', 'chain'), (' raliorroad', 'railroad'), ('yeir', 'year'), ('goign', 'going'), ('ship', 'ship'), (' exspeted', 'expected'), (' pian', 'pain'), (' directuon', 'direction'), (' countires', 'countries'), ('amal', 'MA'), (' Jerey', 'Jersey'), (' wier', 'wear'), (' chartere', 'charter'), (' evoluvion', 'evolution'), (' calyling', 'calling'), (' measurded', 'measured'), (' leerin', 'learn'), (' compused', 'composed'), (' extrmely', 'extremely'), (' secound', 'second'), ('perhapse', 'perhaps'), (' proceeeding', 'proceeding'), (' egid', 'edge'), (' elemnt', 'element'), (' bannad', 'band')]\n"
          ]
        }
      ],
      "source": [
        "# Now text_pairs contains tuples in the format (incorrect, correct)\n",
        "print(\"# rows in training data tuples = \",len(training_data))\n",
        "log_message(f\"# rows in training data = {len(training_data)}\")\n",
        "log_message(f\"############ Random seen data to be validated by the model later.\")\n",
        "\n",
        "seen_validation_data = []\n",
        "number_seen_data_points = 100\n",
        "\n",
        "for i in range (number_seen_data_points):\n",
        "  random_range_start_index = random.randrange(1,30000,1)\n",
        "  for row in training_data[random_range_start_index:random_range_start_index+1]:\n",
        "    #print(row[0])\n",
        "    seen_validation_data.append(row)\n",
        "print(f\"Seen data: {seen_validation_data}\")\n",
        "log_message(f\"Seen data: {seen_validation_data}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yJSJifNN9QU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fffbf14-fc25-406d-cdaf-cf1f82f98f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in unseen validation data = 100\n",
            "Unseen validation data: [('Appel', 'Apple'), ('Booke', 'Book'), ('Kare', 'Car'), ('Dög', 'Dog'), ('Hoise', 'House'), ('Trei', 'Tree'), ('Freind', 'Friend'), ('Hapy', 'Happy'), ('Bieg', 'Big'), ('Littel', 'Little'), ('Rune', 'Run'), ('Jummp', 'Jump'), ('Sée', 'See'), ('Heer', 'Hear'), ('Saiy', 'Say'), ('Goe', 'Go'), ('Comey', 'Come'), ('Stoip', 'Stop'), ('Starit', 'Start'), ('Fynish', 'Finish'), ('Wont', 'Want'), ('Neid', 'Need'), ('Likie', 'Like'), ('Lovve', 'Love'), ('Heelp', 'Help'), ('Tayk', 'Take'), ('Givv', 'Give'), ('Mekke', 'Make'), ('Doe', 'Do'), ('Noow', 'Know'), ('Thinck', 'Think'), ('Fiel', 'Feel'), ('Sée', 'See'), ('Heer', 'Hear'), ('Seee', 'See'), ('Heer', 'Hear'), ('Eit', 'Eat'), ('Drinik', 'Drink'), ('Sleeep', 'Sleep'), ('Wakie', 'Wake'), ('Wurk', 'Work'), ('Plaiy', 'Play'), ('Lernn', 'Learn'), ('Readd', 'Read'), ('Rrite', 'Write'), ('Tawk', 'Talk'), ('Lissten', 'Listen'), ('Smilie', 'Smile'), ('Laught', 'Laugh'), ('Crie', 'Cry'), ('Yelle', 'Yell'), ('Hurrie', 'Hurry'), ('Waite', 'Wait'), ('Hoipe', 'Hope'), ('Wisshe', 'Wish'), ('Trii', 'Try'), ('Fale', 'Fail'), ('Suceede', 'Succeed'), ('Begn', 'Begin'), ('Ennd', 'End'), ('Mornig', 'Morning'), ('Afteernoon', 'Afternoon'), ('Eveninng', 'Evening'), ('Nite', 'Night'), ('Dei', 'Day'), ('Wieck', 'Week'), ('Montn', 'Month'), ('Yeer', 'Year'), ('Centurie', 'Century'), ('Lief', 'Life'), ('Tymme', 'Time'), ('Spase', 'Space'), ('Energee', 'Energy'), ('Watair', 'Water'), ('Firre', 'Fire'), ('Aire', 'Air'), ('Erth', 'Earth'), ('Skye', 'Sky'), ('Sone', 'Sun'), ('Munne', 'Moon'), ('Phenomonal', 'Phenomenal'), ('Compitition', 'Competition'), ('Independacently', 'Independently'), ('Unrelevent', 'Irrelevant'), ('Disapointment', 'Disappointment'), ('Requirments', 'Requirements'), ('Impossiblity', 'Impossibility'), ('Exsceptional', 'Exceptional'), ('Incurrable', 'Incurred'), ('Unnoticable', 'Unnoticeable'), ('Misunderstnding', 'Misunderstanding'), ('Reccomendation', 'Recommendation'), ('Incompatiblity', 'Incompatibility'), ('Exsistential', 'Existential'), ('Irrelevent', 'Irrelevant'), ('Unpredicatable', 'Unpredictable'), ('Disastorous', 'Disastrous'), ('Accomplishement', 'Accomplishment'), ('Inefficiency', 'Inefficacy'), ('Impracticality', 'Impracticability')]\n"
          ]
        }
      ],
      "source": [
        "#Load unseen data by the model\n",
        "unseen_validation_data = [\n",
        "    (\"Appel\", \"Apple\"),\n",
        "    (\"Booke\", \"Book\"),\n",
        "    (\"Kare\", \"Car\"),\n",
        "    (\"Dög\", \"Dog\"),\n",
        "    (\"Hoise\", \"House\"),\n",
        "    (\"Trei\", \"Tree\"),\n",
        "    (\"Freind\", \"Friend\"),\n",
        "    (\"Hapy\", \"Happy\"),\n",
        "    (\"Bieg\", \"Big\"),\n",
        "    (\"Littel\", \"Little\"),\n",
        "    (\"Rune\", \"Run\"),\n",
        "    (\"Jummp\", \"Jump\"),\n",
        "    (\"Sée\", \"See\"),\n",
        "    (\"Heer\", \"Hear\"),\n",
        "    (\"Saiy\", \"Say\"),\n",
        "    (\"Goe\", \"Go\"),\n",
        "    (\"Comey\", \"Come\"),\n",
        "    (\"Stoip\", \"Stop\"),\n",
        "    (\"Starit\", \"Start\"),\n",
        "    (\"Fynish\", \"Finish\"),\n",
        "    (\"Wont\", \"Want\"),\n",
        "    (\"Neid\", \"Need\"),\n",
        "    (\"Likie\", \"Like\"),\n",
        "    (\"Lovve\", \"Love\"),\n",
        "    (\"Heelp\", \"Help\"),\n",
        "    (\"Tayk\", \"Take\"),\n",
        "    (\"Givv\", \"Give\"),\n",
        "    (\"Mekke\", \"Make\"),\n",
        "    (\"Doe\", \"Do\"),\n",
        "    (\"Noow\", \"Know\"),\n",
        "    (\"Thinck\", \"Think\"),\n",
        "    (\"Fiel\", \"Feel\"),\n",
        "    (\"Sée\", \"See\"),\n",
        "    (\"Heer\", \"Hear\"),\n",
        "    (\"Seee\", \"See\"),\n",
        "    (\"Heer\", \"Hear\"),\n",
        "    (\"Eit\", \"Eat\"),\n",
        "    (\"Drinik\", \"Drink\"),\n",
        "    (\"Sleeep\", \"Sleep\"),\n",
        "    (\"Wakie\", \"Wake\"),\n",
        "    (\"Wurk\", \"Work\"),\n",
        "    (\"Plaiy\", \"Play\"),\n",
        "    (\"Lernn\", \"Learn\"),\n",
        "    (\"Readd\", \"Read\"),\n",
        "    (\"Rrite\", \"Write\"),\n",
        "    (\"Tawk\", \"Talk\"),\n",
        "    (\"Lissten\", \"Listen\"),\n",
        "    (\"Smilie\", \"Smile\"),\n",
        "    (\"Laught\", \"Laugh\"),\n",
        "    (\"Crie\", \"Cry\"),\n",
        "    (\"Yelle\", \"Yell\"),\n",
        "    (\"Hurrie\", \"Hurry\"),\n",
        "    (\"Waite\", \"Wait\"),\n",
        "    (\"Hoipe\", \"Hope\"),\n",
        "    (\"Wisshe\", \"Wish\"),\n",
        "    (\"Trii\", \"Try\"),\n",
        "    (\"Fale\", \"Fail\"),\n",
        "    (\"Suceede\", \"Succeed\"),\n",
        "    (\"Begn\", \"Begin\"),\n",
        "    (\"Ennd\", \"End\"),\n",
        "    (\"Mornig\", \"Morning\"),\n",
        "    (\"Afteernoon\", \"Afternoon\"),\n",
        "    (\"Eveninng\", \"Evening\"),\n",
        "    (\"Nite\", \"Night\"),\n",
        "    (\"Dei\", \"Day\"),\n",
        "    (\"Wieck\", \"Week\"),\n",
        "    (\"Montn\", \"Month\"),\n",
        "    (\"Yeer\", \"Year\"),\n",
        "    (\"Centurie\", \"Century\"),\n",
        "    (\"Lief\", \"Life\"),\n",
        "    (\"Tymme\", \"Time\"),\n",
        "    (\"Spase\", \"Space\"),\n",
        "    (\"Energee\", \"Energy\"),\n",
        "    (\"Watair\", \"Water\"),\n",
        "    (\"Firre\", \"Fire\"),\n",
        "    (\"Aire\", \"Air\"),\n",
        "    (\"Erth\", \"Earth\"),\n",
        "    (\"Skye\", \"Sky\"),\n",
        "    (\"Sone\", \"Sun\"),\n",
        "    (\"Munne\", \"Moon\"),\n",
        "    (\"Phenomonal\", \"Phenomenal\"),\n",
        "    (\"Compitition\", \"Competition\"),\n",
        "    (\"Independacently\", \"Independently\"),\n",
        "    (\"Unrelevent\", \"Irrelevant\"),\n",
        "    (\"Disapointment\", \"Disappointment\"),\n",
        "    (\"Requirments\", \"Requirements\"),\n",
        "    (\"Impossiblity\", \"Impossibility\"),\n",
        "    (\"Exsceptional\", \"Exceptional\"),\n",
        "    (\"Incurrable\", \"Incurred\"),\n",
        "    (\"Unnoticable\", \"Unnoticeable\"),\n",
        "    (\"Misunderstnding\", \"Misunderstanding\"),\n",
        "    (\"Reccomendation\", \"Recommendation\"),\n",
        "    (\"Incompatiblity\", \"Incompatibility\"),\n",
        "    (\"Exsistential\", \"Existential\"),\n",
        "    (\"Irrelevent\", \"Irrelevant\"),\n",
        "    (\"Unpredicatable\", \"Unpredictable\"),\n",
        "    (\"Disastorous\", \"Disastrous\"),\n",
        "    (\"Accomplishement\", \"Accomplishment\"),\n",
        "    (\"Inefficiency\", \"Inefficacy\"),\n",
        "    (\"Impracticality\", \"Impracticability\")\n",
        "]\n",
        "\n",
        "log_message(f\"############ Unseen data to be validated by the model later.\")\n",
        "print(f\"Number of rows in unseen validation data = {len(unseen_validation_data)}\")\n",
        "log_message(f\"Number of rows in unseen validation data = {len(unseen_validation_data)}\")\n",
        "\n",
        "\n",
        "print(f\"Unseen validation data: {unseen_validation_data}\")\n",
        "log_message(f\"Unawwn validation data: {unseen_validation_data}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxaML4a-ecLd"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "def preprocess_data(input_texts, target_texts):\n",
        "\n",
        "    #To address the issue of potential non-string types and None values in your input_texts and target_texts\n",
        "    inputs = [task_prefix + str(text) for text in input_texts if text is not None]\n",
        "    target_texts = [str(text) for text in target_texts if text is not None]\n",
        "\n",
        "    # Tokenize the inputs and targets\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    labels = tokenizer(target_texts, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "    return model_inputs, labels\n",
        "\n",
        "# Prepare the training data\n",
        "input_texts, target_texts = zip(*training_data)\n",
        "train_inputs, train_labels = preprocess_data(input_texts, target_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtXGi9wTgMIA"
      },
      "outputs": [],
      "source": [
        "# Inference method\n",
        "def correct_text(input_text):\n",
        "    input_ids = tokenizer.encode(task_prefix + input_text, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(input_ids, max_length=512)[0]\n",
        "    corrected_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "    return corrected_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBC2_GQ-goQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f84062-3692-4360-b030-f3ff610ed670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: Ameriacs--->Richtig: Ameriacs\n",
            "1: countris--->correct: countris\n",
            "2: deie--->correct: correct:\n",
            "3: advnace--->advnace\n",
            "4: secuere--->correct: secuere\n",
            "5: Brtitan--->Brtitan\n",
            "6: polisse--->Richtig: polisse\n",
            "7: suhhc--->: suhhc\n",
            "8: disucss--->correct: disucss\n",
            "9: satifay--->satifay\n",
            "10: facilatie--->correct: facilatie\n",
            "11: Wislon--->Richtig: Wislon\n",
            "12: casjh--->correct: casjh\n",
            "13: raidiatin--->correct: raidiatin\n",
            "14: eductaional--->correct: eductaional\n",
            "15: extnsion--->extnsion\n",
            "16: librayes--->correct: librayes\n",
            "17: Bulletinn--->Bulletinn\n",
            "18:Parelement--->Parelement\n",
            "19: flught--->correct: flught\n",
            "20: foerst--->correct: foerst\n",
            "21: produxe--->correct: produxe\n",
            "22:weither--->: weither\n",
            "23: Afreican--->Afreican\n",
            "24: Treasurry--->Richtig: Treasurry\n",
            "25: patern--->paternal\n",
            "26: rootts--->correct: rootts\n",
            "27: hpoop--->correct:\n",
            "28: temie--->correct: temie\n",
            "29: Devision--->Devision: Devision\n",
            "30: jobj--->correct: jobj\n",
            "31: carrided--->correct: carrided\n",
            "32: evniing--->correct:\n",
            "33: creatiin--->: creatiin\n",
            "34: quarteer--->correct: quarteer\n",
            "35: resltuing--->Richtig: resltuing\n",
            "36: majoirty--->correct: majoirty\n",
            "37: subsescton--->\n",
            "38: woidl--->correct: woidl\n",
            "39: findnings--->correct: findnings\n",
            "40: ellicton--->ellicton\n",
            "41: townees--->correct: townees\n",
            "42: pasend--->correct: pasend\n",
            "43: acceess--->a\n",
            "44: supreoir--->: supreoir\n",
            "45: assissment--->\n",
            "46: seeingg--->correct: seeingg\n",
            "47: proccese--->correct: proccese\n",
            "48: Chiicago--->Chiicago\n",
            "49: wuod--->wuod\n",
            "50: pathe--->correct: pathe\n",
            "51: adedd--->correct:\n",
            "52: themselfe--->: theyselfe\n",
            "53: councel--->correct: councel\n",
            "54: inernal--->correct: correct: inernal\n",
            "55: welldo--->correct: welldo\n",
            "56: publcation--->correct: publcation\n",
            "57: serius--->correct: serius\n",
            "58: tabls--->correct: tabls\n",
            "59: xpect--->correct: xpect\n",
            "60: tethe--->correct: te\n",
            "61:fayth--->correct: fay\n",
            "62: mxied--->correct: mxied\n",
            "63: praxise--->correct correct: praxise\n",
            "64: leeader--->correct: leeader\n",
            "65: filmin--->correct: filmin\n",
            "66: aimee--->correct correct: aimee\n",
            "67: throguhlly--->throguhlly\n",
            "68: srent--->-\n",
            "69: ocupationn--->correct: ocupationn\n",
            "70: subscection--->Korrekt: subscection\n",
            "71: dennyed--->correct: a correct: a correct: a correct: a correct: a correct: a correct: a correct: a correct: a correct: a correct: a correct: a correct: a correct: a correct: a correct:\n",
            "72: becomne--->correct: becomne\n",
            "73:luiking--->correct: luiking\n",
            "74: establisjing--->Richtig: establisjing: establisjing:\n",
            "75: chien--->correct: chien\n",
            "76: raliorroad--->raliorroad\n",
            "77:yeir--->correct: ye\n",
            "78:goign--->: goign\n",
            "79:ship--->correct correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct: correct correct: correct: correct correct: correct: correct correct: correct correct: correct correct: correct correct: correct correct: correct correct: correct correct: correct correct: correct correct\n",
            "80: exspeted--->-\n",
            "81: pian--->correct: pian\n",
            "82: directuon--->correct: directuon\n",
            "83: countires--->correct: countires\n",
            "84:amal--->: amal\n",
            "85: Jerey--->correct: Jerey\n",
            "86: wier--->correct: wier\n",
            "87: chartere--->chartere\n",
            "88: evoluvion--->: evoluvion\n",
            "89: calyling--->: calyling\n",
            "90: measurded--->correct: measurded\n",
            "91: leerin--->correct correct: leerin\n",
            "92: compused--->: compused\n",
            "93: extrmely--->correct:\n",
            "94: secound--->: secound\n",
            "95:perhapse--->: perhapse\n",
            "96: proceeeding--->correct: proceeeding\n",
            "97: egid--->correct: egid\n",
            "98: elemnt--->correct: elemnt\n",
            "99: bannad--->correct: bannad\n",
            "CPU times: user 8min 40s, sys: 262 ms, total: 8min 40s\n",
            "Wall time: 11 s\n"
          ]
        }
      ],
      "source": [
        "#run inference BEFORE finetuning against SEEN data\n",
        "%%time\n",
        "log_message(f\"############ run inference BEFORE finetuning against SEEN data\")\n",
        "for i, row in enumerate(seen_validation_data):\n",
        "  corrected_text = correct_text(row[0])\n",
        "  print(f\"{i}:{row[0]}--->{corrected_text}\")\n",
        "  log_message(f\"{i}:{row[0]}--->{corrected_text}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueauTWNciiMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f4844b-2f81-4482-946a-fec3f9005ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:Appel--->Appel\n",
            "1:Booke--->Booke\n",
            "2:Kare--->Kare\n",
            "3:Dög--->Dög: Dög Dög: Dög Dög: Dög Dög: Dög: Dög: Dög: Dög: Dög: Dög\n",
            "4:Hoise--->Hoise\n",
            "5:Trei--->correct: Trei\n",
            "6:Freind--->Freind: Freind\n",
            "7:Hapy--->Hapy: Hapy\n",
            "8:Bieg--->Bieg\n",
            "9:Littel--->Littel Littel Littel: Littel Littel Littel Littel Littel: Littel\n",
            "10:Rune--->Rune\n",
            "11:Jummp--->Jummp: Jummp Jummp: Jummp Jummp Jummp Jummp: Jummp: Jummp: Jummp Jummp: Jummp\n",
            "12:Sée--->Sé\n",
            "13:Heer--->Heer: Heer: Heer: Heer Heer: Heer\n",
            "14:Saiy--->Saiy\n",
            "15:Goe--->Goe: Goe: Goe: Goe Goe: Goe Goe: Goe: Goe: Goe: Goe: Goe: Goe\n",
            "16:Comey--->Comey\n",
            "17:Stoip--->Stoip\n",
            "18:Starit--->Starit: Starit\n",
            "19:Fynish--->Fynish: Fynish: Fynish: Fynish Fynish: Fynish\n",
            "20:Wont--->Wont\n",
            "21:Neid--->correct: Neid\n",
            "22:Likie--->Likie\n",
            "23:Lovve--->: Lovve\n",
            "24:Heelp--->Heelp\n",
            "25:Tayk--->Tayk\n",
            "26:Givv--->Givv\n",
            "27:Mekke--->Mekke\n",
            "28:Doe--->Doe\n",
            "29:Noow--->Noow\n",
            "30:Thinck--->: Thinck\n",
            "31:Fiel--->Fiel\n",
            "32:Sée--->Sé\n",
            "33:Heer--->Heer: Heer: Heer: Heer Heer: Heer\n",
            "34:Seee--->Seee\n",
            "35:Heer--->Heer: Heer: Heer: Heer Heer: Heer\n",
            "36:Eit--->Eit: Eit: Eit: Eit: Eit: Eit\n",
            "37:Drinik--->correct: Drinik\n",
            "38:Sleeep--->correct: Sleep\n",
            "39:Wakie--->: Wakie\n",
            "40:Wurk--->: Wurk\n",
            "41:Plaiy--->correct: Plaiy\n",
            "42:Lernn--->Lernn\n",
            "43:Readd--->readd\n",
            "44:Rrite--->correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct correct\n",
            "45:Tawk--->: Tawk\n",
            "46:Lissten--->Richtig: Lissten\n",
            "47:Smilie--->Smilie\n",
            "48:Laught--->Laught: Laught Laught: Laught Laught: Laught\n",
            "49:Crie--->Crie\n",
            "50:Yelle--->Yelle\n",
            "51:Hurrie--->: Hurrie\n",
            "52:Waite--->: Waite\n",
            "53:Hoipe--->: Hoipe\n",
            "54:Wisshe--->Richtig: Wisshe\n",
            "55:Trii--->a : Trii\n",
            "56:Fale--->False\n",
            "57:Suceede--->Suceede\n",
            "58:Begn--->Begn\n",
            "59:Ennd--->correct correct: Ennd\n",
            "60:Mornig--->Mornig\n",
            "61:Afteernoon--->Richtig: Afteerno\n",
            "62:Eveninng--->Eveninng\n",
            "63:Nite--->Nite\n",
            "64:Dei--->correct: Dei\n",
            "65:Wieck--->Wieck\n",
            "66:Montn--->Montn\n",
            "67:Yeer--->correct: Yeer\n",
            "68:Centurie--->Centurie\n",
            "69:Lief--->Lief\n",
            "70:Tymme--->: Tymme\n",
            "71:Spase--->Richtig: Spase\n",
            "72:Energee--->correct: Energee\n",
            "73:Watair--->Watair\n",
            "74:Firre--->Firre\n",
            "75:Aire--->Aire\n",
            "76:Erth--->Erth\n",
            "77:Skye--->Skye\n",
            "78:Sone--->Sone\n",
            "79:Munne--->Munne\n",
            "80:Phenomonal--->correct correct: Phenomonal\n",
            "81:Compitition--->Compitition\n",
            "82:Independacently--->correct:\n",
            "83:Unrelevent--->Unrelevent\n",
            "84:Disapointment--->Désignation\n",
            "85:Requirments--->Kor correct: Exquirments\n",
            "86:Impossiblity--->Richtig: Impossiblity\n",
            "87:Exsceptional--->Richtig: Exsceptional\n",
            "88:Incurrable--->: Incurrable\n",
            "89:Unnoticable--->Unnoticable\n",
            "90:Misunderstnding--->Richtig: Fehl\n",
            "91:Reccomendation--->Reccomendation\n",
            "92:Incompatiblity--->Incompatiblity\n",
            "93:Exsistential--->Richtig: Exsistential\n",
            "94:Irrelevent--->Irrelevent\n",
            "95:Unpredicatable--->Unpredictable\n",
            "96:Disastorous--->: Disastorous\n",
            "97:Accomplishement--->Kor\n",
            "98:Inefficiency--->Ineffizienz\n",
            "99:Impracticality--->Impractical\n",
            "CPU times: user 9min 34s, sys: 336 ms, total: 9min 34s\n",
            "Wall time: 12.1 s\n"
          ]
        }
      ],
      "source": [
        "#run inference BEFORE finetuning against UNSEEN data\n",
        "%%time\n",
        "log_message(f\"############ run inference BEFORE finetuning against UNSEEN data\")\n",
        "for i, row in enumerate(unseen_validation_data):\n",
        "  corrected_text = correct_text(row[0])\n",
        "  print(f\"{i}:{row[0]}--->{corrected_text}\")\n",
        "  log_message(f\"{i}:{row[0]}--->{corrected_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGHMV6ZsPtR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30fda84c-475e-4903-ce9a-eb54f2776dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training rows per batch: 2577\n"
          ]
        }
      ],
      "source": [
        "#fine tuning w/ batch procerssing\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset = TensorDataset(train_inputs[\"input_ids\"], train_inputs[\"attention_mask\"], train_labels)\n",
        "\n",
        "# Create a DataLoader\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "print(\"# training rows per batch:\", len(train_dataloader))\n",
        "log_message(f\"# training rows per batch = {len(train_dataloader)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyRyeMxePtR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e7db883-0cf6-463a-d902-e72e61b8085e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-02 09:42:52 Started batch #1/2577 in epoch #1/5\n",
            "2024-05-02 09:42:58 Finished batch #1/2577 in epoch #1/5 in (5.63 seconds)\n",
            "2024-05-02 09:42:58 Started batch #2/2577 in epoch #1/5\n",
            "2024-05-02 09:43:03 Finished batch #2/2577 in epoch #1/5 in (5.79 seconds)\n",
            "2024-05-02 09:43:03 Started batch #3/2577 in epoch #1/5\n",
            "2024-05-02 09:43:09 Finished batch #3/2577 in epoch #1/5 in (5.93 seconds)\n",
            "2024-05-02 09:43:09 Started batch #4/2577 in epoch #1/5\n",
            "2024-05-02 09:43:15 Finished batch #4/2577 in epoch #1/5 in (5.89 seconds)\n",
            "2024-05-02 09:43:15 Started batch #5/2577 in epoch #1/5\n",
            "2024-05-02 09:43:21 Finished batch #5/2577 in epoch #1/5 in (5.68 seconds)\n",
            "2024-05-02 09:43:21 Started batch #6/2577 in epoch #1/5\n",
            "2024-05-02 09:43:27 Finished batch #6/2577 in epoch #1/5 in (6.27 seconds)\n",
            "2024-05-02 09:43:27 Started batch #7/2577 in epoch #1/5\n",
            "2024-05-02 09:43:33 Finished batch #7/2577 in epoch #1/5 in (5.76 seconds)\n",
            "2024-05-02 09:43:33 Started batch #8/2577 in epoch #1/5\n",
            "2024-05-02 09:43:39 Finished batch #8/2577 in epoch #1/5 in (6.04 seconds)\n",
            "2024-05-02 09:43:39 Started batch #9/2577 in epoch #1/5\n",
            "2024-05-02 09:43:45 Finished batch #9/2577 in epoch #1/5 in (5.61 seconds)\n",
            "2024-05-02 09:43:45 Started batch #10/2577 in epoch #1/5\n",
            "2024-05-02 09:43:51 Finished batch #10/2577 in epoch #1/5 in (6.09 seconds)\n",
            "2024-05-02 09:43:51 Started batch #11/2577 in epoch #1/5\n",
            "2024-05-02 09:43:56 Finished batch #11/2577 in epoch #1/5 in (5.74 seconds)\n",
            "2024-05-02 09:43:56 Started batch #12/2577 in epoch #1/5\n",
            "2024-05-02 09:44:02 Finished batch #12/2577 in epoch #1/5 in (5.76 seconds)\n",
            "2024-05-02 09:44:02 Started batch #13/2577 in epoch #1/5\n",
            "2024-05-02 09:44:08 Finished batch #13/2577 in epoch #1/5 in (6.07 seconds)\n",
            "2024-05-02 09:44:08 Started batch #14/2577 in epoch #1/5\n",
            "2024-05-02 09:44:14 Finished batch #14/2577 in epoch #1/5 in (5.58 seconds)\n",
            "2024-05-02 09:44:14 Started batch #15/2577 in epoch #1/5\n",
            "2024-05-02 09:44:20 Finished batch #15/2577 in epoch #1/5 in (5.70 seconds)\n",
            "2024-05-02 09:44:20 Started batch #16/2577 in epoch #1/5\n",
            "2024-05-02 09:44:25 Finished batch #16/2577 in epoch #1/5 in (5.89 seconds)\n",
            "2024-05-02 09:44:25 Started batch #17/2577 in epoch #1/5\n",
            "2024-05-02 09:44:31 Finished batch #17/2577 in epoch #1/5 in (5.79 seconds)\n",
            "2024-05-02 09:44:31 Started batch #18/2577 in epoch #1/5\n",
            "2024-05-02 09:44:37 Finished batch #18/2577 in epoch #1/5 in (6.23 seconds)\n",
            "2024-05-02 09:44:37 Started batch #19/2577 in epoch #1/5\n",
            "2024-05-02 09:44:43 Finished batch #19/2577 in epoch #1/5 in (5.88 seconds)\n",
            "2024-05-02 09:44:43 Started batch #20/2577 in epoch #1/5\n",
            "2024-05-02 09:44:50 Finished batch #20/2577 in epoch #1/5 in (6.24 seconds)\n",
            "2024-05-02 09:44:50 Started batch #21/2577 in epoch #1/5\n",
            "2024-05-02 09:44:56 Finished batch #21/2577 in epoch #1/5 in (6.06 seconds)\n",
            "2024-05-02 09:44:56 Started batch #22/2577 in epoch #1/5\n",
            "2024-05-02 09:45:01 Finished batch #22/2577 in epoch #1/5 in (5.66 seconds)\n",
            "2024-05-02 09:45:01 Started batch #23/2577 in epoch #1/5\n",
            "2024-05-02 09:45:07 Finished batch #23/2577 in epoch #1/5 in (5.94 seconds)\n",
            "2024-05-02 09:45:07 Started batch #24/2577 in epoch #1/5\n",
            "2024-05-02 09:45:13 Finished batch #24/2577 in epoch #1/5 in (5.80 seconds)\n",
            "2024-05-02 09:45:13 Started batch #25/2577 in epoch #1/5\n",
            "2024-05-02 09:45:19 Finished batch #25/2577 in epoch #1/5 in (6.21 seconds)\n",
            "2024-05-02 09:45:19 Started batch #26/2577 in epoch #1/5\n",
            "2024-05-02 09:45:25 Finished batch #26/2577 in epoch #1/5 in (5.80 seconds)\n",
            "2024-05-02 09:45:25 Started batch #27/2577 in epoch #1/5\n",
            "2024-05-02 09:45:31 Finished batch #27/2577 in epoch #1/5 in (6.02 seconds)\n",
            "2024-05-02 09:45:31 Started batch #28/2577 in epoch #1/5\n",
            "2024-05-02 09:45:38 Finished batch #28/2577 in epoch #1/5 in (6.39 seconds)\n",
            "2024-05-02 09:45:38 Started batch #29/2577 in epoch #1/5\n",
            "2024-05-02 09:45:43 Finished batch #29/2577 in epoch #1/5 in (5.82 seconds)\n",
            "2024-05-02 09:45:43 Started batch #30/2577 in epoch #1/5\n",
            "2024-05-02 09:45:50 Finished batch #30/2577 in epoch #1/5 in (6.29 seconds)\n",
            "2024-05-02 09:45:50 Started batch #31/2577 in epoch #1/5\n",
            "2024-05-02 09:45:55 Finished batch #31/2577 in epoch #1/5 in (5.69 seconds)\n",
            "2024-05-02 09:45:55 Started batch #32/2577 in epoch #1/5\n",
            "2024-05-02 09:46:02 Finished batch #32/2577 in epoch #1/5 in (6.13 seconds)\n",
            "2024-05-02 09:46:02 Started batch #33/2577 in epoch #1/5\n",
            "2024-05-02 09:46:08 Finished batch #33/2577 in epoch #1/5 in (5.96 seconds)\n",
            "2024-05-02 09:46:08 Started batch #34/2577 in epoch #1/5\n",
            "2024-05-02 09:46:14 Finished batch #34/2577 in epoch #1/5 in (6.02 seconds)\n",
            "2024-05-02 09:46:14 Started batch #35/2577 in epoch #1/5\n",
            "2024-05-02 09:46:20 Finished batch #35/2577 in epoch #1/5 in (6.48 seconds)\n",
            "2024-05-02 09:46:20 Started batch #36/2577 in epoch #1/5\n",
            "2024-05-02 09:46:26 Finished batch #36/2577 in epoch #1/5 in (6.12 seconds)\n",
            "2024-05-02 09:46:26 Started batch #37/2577 in epoch #1/5\n",
            "2024-05-02 09:46:32 Finished batch #37/2577 in epoch #1/5 in (6.05 seconds)\n",
            "2024-05-02 09:46:32 Started batch #38/2577 in epoch #1/5\n",
            "2024-05-02 09:46:38 Finished batch #38/2577 in epoch #1/5 in (6.03 seconds)\n",
            "2024-05-02 09:46:38 Started batch #39/2577 in epoch #1/5\n",
            "2024-05-02 09:46:44 Finished batch #39/2577 in epoch #1/5 in (6.22 seconds)\n",
            "2024-05-02 09:46:44 Started batch #40/2577 in epoch #1/5\n",
            "2024-05-02 09:46:50 Finished batch #40/2577 in epoch #1/5 in (5.72 seconds)\n",
            "2024-05-02 09:46:50 Started batch #41/2577 in epoch #1/5\n",
            "2024-05-02 09:46:56 Finished batch #41/2577 in epoch #1/5 in (5.94 seconds)\n",
            "2024-05-02 09:46:56 Started batch #42/2577 in epoch #1/5\n",
            "2024-05-02 09:47:02 Finished batch #42/2577 in epoch #1/5 in (6.04 seconds)\n",
            "2024-05-02 09:47:02 Started batch #43/2577 in epoch #1/5\n",
            "2024-05-02 09:47:08 Finished batch #43/2577 in epoch #1/5 in (6.26 seconds)\n",
            "2024-05-02 09:47:08 Started batch #44/2577 in epoch #1/5\n",
            "2024-05-02 09:47:15 Finished batch #44/2577 in epoch #1/5 in (6.12 seconds)\n",
            "2024-05-02 09:47:15 Started batch #45/2577 in epoch #1/5\n",
            "2024-05-02 09:47:20 Finished batch #45/2577 in epoch #1/5 in (5.42 seconds)\n",
            "2024-05-02 09:47:20 Started batch #46/2577 in epoch #1/5\n",
            "2024-05-02 09:47:26 Finished batch #46/2577 in epoch #1/5 in (6.11 seconds)\n",
            "2024-05-02 09:47:26 Started batch #47/2577 in epoch #1/5\n",
            "2024-05-02 09:47:32 Finished batch #47/2577 in epoch #1/5 in (5.91 seconds)\n",
            "2024-05-02 09:47:32 Started batch #48/2577 in epoch #1/5\n",
            "2024-05-02 09:47:38 Finished batch #48/2577 in epoch #1/5 in (6.07 seconds)\n",
            "2024-05-02 09:47:38 Started batch #49/2577 in epoch #1/5\n",
            "2024-05-02 09:47:44 Finished batch #49/2577 in epoch #1/5 in (5.72 seconds)\n",
            "2024-05-02 09:47:44 Started batch #50/2577 in epoch #1/5\n",
            "2024-05-02 09:47:50 Finished batch #50/2577 in epoch #1/5 in (5.75 seconds)\n",
            "2024-05-02 09:47:50 Started batch #51/2577 in epoch #1/5\n",
            "2024-05-02 09:47:56 Finished batch #51/2577 in epoch #1/5 in (6.26 seconds)\n",
            "2024-05-02 09:47:56 Started batch #52/2577 in epoch #1/5\n",
            "2024-05-02 09:48:02 Finished batch #52/2577 in epoch #1/5 in (6.13 seconds)\n",
            "2024-05-02 09:48:02 Started batch #53/2577 in epoch #1/5\n",
            "2024-05-02 09:48:08 Finished batch #53/2577 in epoch #1/5 in (5.91 seconds)\n",
            "2024-05-02 09:48:08 Started batch #54/2577 in epoch #1/5\n",
            "2024-05-02 09:48:14 Finished batch #54/2577 in epoch #1/5 in (5.98 seconds)\n",
            "2024-05-02 09:48:14 Started batch #55/2577 in epoch #1/5\n",
            "2024-05-02 09:48:20 Finished batch #55/2577 in epoch #1/5 in (5.73 seconds)\n",
            "2024-05-02 09:48:20 Started batch #56/2577 in epoch #1/5\n",
            "2024-05-02 09:48:26 Finished batch #56/2577 in epoch #1/5 in (6.11 seconds)\n",
            "2024-05-02 09:48:26 Started batch #57/2577 in epoch #1/5\n",
            "2024-05-02 09:48:32 Finished batch #57/2577 in epoch #1/5 in (6.04 seconds)\n",
            "2024-05-02 09:48:32 Started batch #58/2577 in epoch #1/5\n",
            "2024-05-02 09:48:38 Finished batch #58/2577 in epoch #1/5 in (6.36 seconds)\n",
            "2024-05-02 09:48:38 Started batch #59/2577 in epoch #1/5\n",
            "2024-05-02 09:48:44 Finished batch #59/2577 in epoch #1/5 in (6.10 seconds)\n",
            "2024-05-02 09:48:44 Started batch #60/2577 in epoch #1/5\n",
            "2024-05-02 09:48:51 Finished batch #60/2577 in epoch #1/5 in (6.29 seconds)\n",
            "2024-05-02 09:48:51 Started batch #61/2577 in epoch #1/5\n",
            "2024-05-02 09:48:56 Finished batch #61/2577 in epoch #1/5 in (5.88 seconds)\n",
            "2024-05-02 09:48:56 Started batch #62/2577 in epoch #1/5\n",
            "2024-05-02 09:49:03 Finished batch #62/2577 in epoch #1/5 in (6.22 seconds)\n",
            "2024-05-02 09:49:03 Started batch #63/2577 in epoch #1/5\n",
            "2024-05-02 09:49:09 Finished batch #63/2577 in epoch #1/5 in (6.12 seconds)\n",
            "2024-05-02 09:49:09 Started batch #64/2577 in epoch #1/5\n",
            "2024-05-02 09:49:15 Finished batch #64/2577 in epoch #1/5 in (5.98 seconds)\n",
            "2024-05-02 09:49:15 Started batch #65/2577 in epoch #1/5\n",
            "2024-05-02 09:49:21 Finished batch #65/2577 in epoch #1/5 in (6.16 seconds)\n",
            "2024-05-02 09:49:21 Started batch #66/2577 in epoch #1/5\n",
            "2024-05-02 09:49:27 Finished batch #66/2577 in epoch #1/5 in (5.87 seconds)\n",
            "2024-05-02 09:49:27 Started batch #67/2577 in epoch #1/5\n",
            "2024-05-02 09:49:33 Finished batch #67/2577 in epoch #1/5 in (6.54 seconds)\n",
            "2024-05-02 09:49:33 Started batch #68/2577 in epoch #1/5\n",
            "2024-05-02 09:49:39 Finished batch #68/2577 in epoch #1/5 in (5.69 seconds)\n",
            "2024-05-02 09:49:39 Started batch #69/2577 in epoch #1/5\n",
            "2024-05-02 09:49:45 Finished batch #69/2577 in epoch #1/5 in (6.28 seconds)\n",
            "2024-05-02 09:49:45 Started batch #70/2577 in epoch #1/5\n",
            "2024-05-02 09:49:52 Finished batch #70/2577 in epoch #1/5 in (6.28 seconds)\n",
            "2024-05-02 09:49:52 Started batch #71/2577 in epoch #1/5\n",
            "2024-05-02 09:49:58 Finished batch #71/2577 in epoch #1/5 in (6.21 seconds)\n",
            "2024-05-02 09:49:58 Started batch #72/2577 in epoch #1/5\n",
            "2024-05-02 09:50:04 Finished batch #72/2577 in epoch #1/5 in (6.19 seconds)\n",
            "2024-05-02 09:50:04 Started batch #73/2577 in epoch #1/5\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune the model w/ frequent output\n",
        "#%%time\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    current_batch = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch_start_time = datetime.datetime.now()\n",
        "        print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Started batch #{current_batch+1}/{len(train_dataloader)} in epoch #{epoch+1}/{num_epochs}\")\n",
        "        log_message(f\"Started batch #{current_batch+1}/{len(train_dataloader)} in epoch #{epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Unpack the batch\n",
        "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
        "\n",
        "\n",
        "        # Zero the gradients on each iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
        "\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        batch_end_time = datetime.datetime.now()\n",
        "        print(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Finished batch #{current_batch+1}/{len(train_dataloader)} in epoch #{epoch+1}/{num_epochs} in ({(batch_end_time - batch_start_time).total_seconds():.2f} seconds)\")\n",
        "        log_message(f\"Finished batch #{current_batch+1}/{len(train_dataloader)} in epoch #{epoch+1}/{num_epochs}  in ({(batch_end_time - batch_start_time).total_seconds():.2f} seconds)\")\n",
        "\n",
        "        current_batch += 1\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate average loss over the number of batches\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "    log_message(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW7hjDL6mPmN"
      },
      "outputs": [],
      "source": [
        "#run inference AFTER finetuning against SEEN data\n",
        "%%time\n",
        "log_message(f\"############ run inference AFTER finetuning against SEEN data\")\n",
        "for i, row in enumerate(seen_validation_data):\n",
        "  corrected_text = correct_text(row[0])\n",
        "  print(f\"{i}:{row[0]}--->{corrected_text} vs. {row[1]} \")\n",
        "  log_message(f\"{i}:{row[0]}--->{corrected_text} vs. {row[1]}  \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIt17-R-PtR6"
      },
      "outputs": [],
      "source": [
        "#run inference AFTER finetuning against UNSEEN data\n",
        "%%time\n",
        "log_message(f\"############ run inference AFTER finetuning against UNSEEN data\")\n",
        "for i, row in enumerate(unseen_validation_data):\n",
        "  corrected_text = correct_text(row[0])\n",
        "  print(f\"{i}:{row[0]}--->{corrected_text} vs. {row[1]} \")\n",
        "  log_message(f\"{i}:{row[0]}--->{corrected_text} vs. {row[1]} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB_IHn2QoPoF"
      },
      "outputs": [],
      "source": [
        "# Save the model and tokenizer\n",
        "#Save model to Local Drive\n",
        "model.save_pretrained(f\"/content/drive/My Drive/Startup Projects/auto-correct/models/{finetuning_iteration}\")\n",
        "tokenizer.save_pretrained(f\"/content/drive/My Drive/Startup Projects/auto-correct/models/{finetuning_iteration}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1H18epToXfT"
      },
      "outputs": [],
      "source": [
        "# How to load the model from drive\n",
        "\n",
        "finetuned_model_path = f\"/content/drive/My Drive/Startup Projects/auto-correct/models/{finetuning_iteration}\"\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(finetuned_model_path)\n",
        "model_finetuned_spelling_en_4000_words = T5ForConditionalGeneration.from_pretrained(finetuned_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD6iJhKDpwg2"
      },
      "outputs": [],
      "source": [
        "#use the finetuned model.\n",
        "def correct_spelling_after_fine_tuning(word):\n",
        "    input_ids = tokenizer.encode(\"correct:\" + word, return_tensors=\"pt\")\n",
        "\n",
        "    # Correct spelling\n",
        "    outputs = model_finetuned_spelling_en_4000_words.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the output\n",
        "    correction = finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return correction\n",
        "\n",
        "# Example text\n",
        "word = \"givie\"\n",
        "correction = correct_spelling_after_fine_tuning(word)\n",
        "print(\"Correction:\", correction)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "log_message(f\"############ run inference AFTER finetuning against SEEN data using LOCAL model\")\n",
        "for i, row in enumerate(seen_validation_data):\n",
        "  corrected_text = correct_spelling_after_fine_tuning(row[0])\n",
        "  print(f\"{i}:{row[0]}--->{corrected_text} vs. {row[1]}\")\n",
        "  log_message(f\"{i}:{row[0]}--->{corrected_text}\")"
      ],
      "metadata": {
        "id": "ivQv5PLP22rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "log_message(f\"############ run inference AFTER finetuning against UNSEEN data using LOCAL model\")\n",
        "for i, row in enumerate(unseen_validation_data):\n",
        "  corrected_text = correct_spelling_after_fine_tuning(row[0])\n",
        "  print(f\"{i}:{row[0]}--->{corrected_text} vs. {row[1]}\")\n",
        "  log_message(f\"{i}:{row[0]}--->{corrected_text}\")"
      ],
      "metadata": {
        "id": "yofe7r6Y2d69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwEcka7L21eQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}